{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--dataset DATASET]\n",
      "                             [--datasets_dir DATASETS_DIR]\n",
      "                             [--load_size LOAD_SIZE] [--crop_size CROP_SIZE]\n",
      "                             [--batch_size BATCH_SIZE] [--epochs EPOCHS]\n",
      "                             [--epoch_decay EPOCH_DECAY] [--lr LR]\n",
      "                             [--beta_1 BETA_1]\n",
      "                             [--adversarial_loss_mode {gan,hinge_v1,hinge_v2,lsgan,wgan}]\n",
      "                             [--gradient_penalty_mode {none,dragan,wgan-gp}]\n",
      "                             [--gradient_penalty_weight GRADIENT_PENALTY_WEIGHT]\n",
      "                             [--cycle_loss_weight CYCLE_LOSS_WEIGHT]\n",
      "                             [--identity_loss_weight IDENTITY_LOSS_WEIGHT]\n",
      "                             [--pool_size POOL_SIZE]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\M.RINEETH\\AppData\\Roaming\\jupyter\\runtime\\kernel-411c46e7-71e0-46ed-aeec-6d2c044a75ac.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\M.RINEETH\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3351: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "\n",
    "import imlib as im\n",
    "import numpy as np\n",
    "import pylib as py\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tf2lib as tl\n",
    "import tf2gan as gan\n",
    "from PIL import Image\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import data\n",
    "import random\n",
    "import module\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# =                                   param                                    =\n",
    "# ==============================================================================\n",
    "\n",
    "'''py.arg('--dataset', default='summer2winter_yosemite')\n",
    "py.arg('--datasets_dir', default='dataset')\n",
    "py.arg('--load_size', type=int, default=256)  # load image to this size\n",
    "py.arg('--crop_size', type=int, default=256)  # then crop to this size\n",
    "py.arg('--batch_size', type=int, default=1)\n",
    "py.arg('--epochs', type=int, default=200)\n",
    "py.arg('--epoch_decay', type=int, default=100)  # epoch to start decaying learning rate\n",
    "py.arg('--lr', type=float, default=0.0002)\n",
    "py.arg('--beta_1', type=float, default=0.5)\n",
    "py.arg('--adversarial_loss_mode', default='lsgan', choices=['gan', 'hinge_v1', 'hinge_v2', 'lsgan', 'wgan'])\n",
    "py.arg('--gradient_penalty_mode', default='none', choices=['none', 'dragan', 'wgan-gp'])\n",
    "py.arg('--gradient_penalty_weight', type=float, default=10.0)\n",
    "py.arg('--cycle_loss_weight', type=float, default=10.0)\n",
    "py.arg('--identity_loss_weight', type=float, default=0.0)\n",
    "py.arg('--pool_size', type=int, default=50)  # pool size to store fake samples\n",
    "args = py.args()'''\n",
    "adataset='hair'\n",
    "aload_size=\n",
    "acrop_size=\n",
    "abatch_size=\n",
    "apool_size=\n",
    "aadversarial_loss_mode=\n",
    "aepochs=\n",
    "agradient_penalty_weight=\n",
    "alr=\n",
    "aepoch_decay=\n",
    "abeta_1=\n",
    "acycle_loss_weight=\n",
    "aidentity_loss_weight\n",
    "\n",
    "\n",
    "\n",
    "# output_dir\n",
    "output_dir = py.join('output', adataset)\n",
    "py.mkdir(output_dir)\n",
    "\n",
    "# save settings\n",
    "#py.args_to_yaml(py.join(output_dir, 'settings.yml'), args)\n",
    "\n",
    "def createdata(l):\n",
    "    n=len(l)\n",
    "    lis=[]\n",
    "    for k in range(n):\n",
    "        s=l[k].split('\\\\')[-1]\n",
    "        s2='data\\\\tests\\\\0.1_color\\\\'+s\n",
    "        lis.append(s2)\n",
    "    return lis\n",
    "\n",
    "# ==============================================================================\n",
    "# =                                    data                                    =\n",
    "# ==============================================================================\n",
    "\n",
    "A_imgs = py.glob(py.join('data\\\\tests', 'trainA'), '*.jpg')\n",
    "len_dataset=len(A_imgs)\n",
    "\n",
    "\n",
    "A_color=createdata(A_imgs)\n",
    "A_Set=data.make_dataset( A_imgs,A_color,abatch_size, aload_size, acrop_size, training=False,work=0)\n",
    "\n",
    "B_imgs = py.glob(py.join('data\\\\tests', 'trainB'), '*.jpg')\n",
    "B_color=createdata(B_imgs)\n",
    "B_set=data.make_dataset( B_imgs,B_color,abatch_size, aload_size, acrop_size, training=False,work=0)\n",
    "\n",
    "B_lis=list(B_set.as_numpy_iterator())\n",
    "B_list=[]\n",
    "for b in B_lis:\n",
    "    m,=b\n",
    "    B_list.append(tf.convert_to_tensor(m))\n",
    "\n",
    "B_length=len(B_list)\n",
    "#print('b list\\n',B_list[0][0].squeeze().shape)\n",
    "\n",
    "dev = py.glob(py.join('data\\\\tests', 'dev'), '*.jpg')\n",
    "dev_color=createdata(dev)\n",
    "devset=data.make_dataset( dev,dev_color,abatch_size, aload_size, acrop_size, training=False,work=0)\n",
    "\n",
    "test_imgs = py.glob(py.join('data\\\\tests', 'test'), '*.jpg')\n",
    "test_color=createdata(test_imgs)\n",
    "test_set=data.make_dataset( test_imgs,test_color,abatch_size, aload_size, acrop_size, training=False,work=0)\n",
    "test_lis=list(test_set.as_numpy_iterator())\n",
    "test_list=[]\n",
    "for b in test_lis:\n",
    "    m,=b\n",
    "    test_list.append(tf.convert_to_tensor(m))\n",
    "\n",
    "\n",
    "test_length=len(test_list)\n",
    "\n",
    "\n",
    "A2B_pool = data.ItemPool(apool_size)\n",
    "#B2A_pool = data.ItemPool(a.pool_size)\n",
    "\n",
    "print('session starts \\n')\n",
    "print('A ',len_dataset)\n",
    "print('B',B_length)\n",
    "print('test ',test_length)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# =                                   models                                   =\n",
    "# ==============================================================================\n",
    "\n",
    "G = module.ResnetGenerator(input_shape=(acrop_size, acrop_size, 6))\n",
    "\n",
    "D= module.ConvDiscriminator(input_shape=(acrop_size, acrop_size, 6))\n",
    "#print('generator')\n",
    "#print(G.summary())\n",
    "\n",
    "#print('discriminator')\n",
    "#print(D.summary())\n",
    "\n",
    "\n",
    "d_loss_fn, g_loss_fn = gan.get_adversarial_losses_fn(aadversarial_loss_mode)\n",
    "cycle_loss_fn = tf.losses.MeanAbsoluteError()\n",
    "identity_loss_fn = tf.losses.MeanAbsoluteError()\n",
    "\n",
    "G_lr_scheduler = module.LinearDecay(alr, aepochs * len_dataset, aepoch_decay * len_dataset)\n",
    "D_lr_scheduler = module.LinearDecay(alr, aepochs * len_dataset, aepoch_decay * len_dataset)\n",
    "G_optimizer = keras.optimizers.Adam(learning_rate=G_lr_scheduler, beta_1=abeta_1)\n",
    "D_optimizer = keras.optimizers.Adam(learning_rate=D_lr_scheduler, beta_1=abeta_1)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# =                                 train step                                 =\n",
    "# ==============================================================================\n",
    "\n",
    "@tf.function\n",
    "def train_G(A,B):\n",
    "    with tf.GradientTape() as t:\n",
    "\n",
    "        At=A[:,:,:,3:]\n",
    "        Bt=B[:,:,:,3:]\n",
    "        Ai=A[:,:,:,:3]\n",
    "        Bi=B[:,:,:,:3]\n",
    "\n",
    "        A=tf.concat([Ai,Bt],axis=3)\n",
    "\n",
    "        A2B = G(A, training=True)\n",
    "        A2B=tf.concat([A2B,At],axis=3)\n",
    "        A2B2A = G(A2B, training=True)\n",
    "        B2B = G(B, training=True)\n",
    "\n",
    "        A2B=tf.concat([A2B[:,:,:,:3],Bt])\n",
    "        A2B_d_logits = D(A2B, training=True)\n",
    "\n",
    "        A2B_g_loss = g_loss_fn(A2B_d_logits)\n",
    "        A2B2A_cycle_loss = cycle_loss_fn(Ai, A2B2A)\n",
    "        B2B_id_loss = identity_loss_fn(Bi, B2B)\n",
    "\n",
    "        G_loss = A2B_g_loss  + A2B2A_cycle_loss  * acycle_loss_weight   + B2B_id_loss * aidentity_loss_weight\n",
    "\n",
    "    G_grad = t.gradient(G_loss, G.trainable_variables )\n",
    "    G_optimizer.apply_gradients(zip(G_grad, G.trainable_variables ))\n",
    "\n",
    "    return A2B, {'A2B_g_loss': A2B_g_loss,\n",
    "                      'A2B2A_cycle_loss': A2B2A_cycle_loss,\n",
    "                      'B2B_id_loss': B2B_id_loss}\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_D(B, A2B):\n",
    "    with tf.GradientTape() as t:\n",
    "        B_d_logits = D(B, training=True)\n",
    "        A2B_d_logits = D(A2B, training=True)\n",
    "\n",
    "        B_d_loss, A2B_d_loss = d_loss_fn(B_d_logits, A2B_d_logits)\n",
    "        #D_gp = gan.gradient_penalty(functools.partial(D, training=True), B, A2B, mode=args.gradient_penalty_mode)\n",
    "\n",
    "        D_loss = B_d_loss + A2B_d_loss #+ D_gp * agradient_penalty_weight\n",
    "\n",
    "    D_grad = t.gradient(D_loss, D.trainable_variables)\n",
    "    D_optimizer.apply_gradients(zip(D_grad, D.trainable_variables ))\n",
    "\n",
    "    return {'B_d_loss': B_d_loss + A2B_d_loss,\n",
    "            'D_gp': D_gp}\n",
    "\n",
    "\n",
    "def train_step(A, B):\n",
    "    A2B, G_loss_dict = train_G(A, B)\n",
    "\n",
    "    # cannot autograph `A2B_pool`\n",
    "    #A2B = A2B_pool(A2B)  # or A2B = A2B_pool(A2B.numpy()), but it is much slower\n",
    "\n",
    "    D_loss_dict = train_D( B, A2B)\n",
    "\n",
    "    return G_loss_dict, D_loss_dict\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def sample(A,B):\n",
    "    At = A[0, :, :, 3:]\n",
    "    Bt = B[0, :, :, 3:]\n",
    "    Ai = A[0, :, :, :3]\n",
    "    Bi = B[0, :, :, :3]\n",
    "\n",
    "    A = tf.concat([Ai, Bt], axis=2)\n",
    "\n",
    "    A2B = G(A, training=False)\n",
    "    A2B = tf.concat([A2B, At], axis=2)\n",
    "    A2B2A = G(A2B, training=False)\n",
    "\n",
    "\n",
    "    return Ai,Bt,A2B, A2B2A\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# =                                    run                                     =\n",
    "# ==============================================================================\n",
    "\n",
    "# epoch counter\n",
    "ep_cnt = tf.Variable(initial_value=0, trainable=False, dtype=tf.int64)\n",
    "\n",
    "# checkpoint\n",
    "checkpoint = tl.Checkpoint(dict(G=G,\n",
    "                                D=D,\n",
    "                                G_optimizer=G_optimizer,\n",
    "                                D_optimizer=D_optimizer,\n",
    "                                ep_cnt=ep_cnt),\n",
    "                           py.join(output_dir, 'checkpoints'),\n",
    "                           max_to_keep=5)\n",
    "try:  # restore checkpoint including the epoch counter\n",
    "    checkpoint.restore().assert_existing_objects_matched()\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "# summary\n",
    "train_summary_writer = tf.summary.create_file_writer(py.join(output_dir, 'summaries', 'train'))\n",
    "\n",
    "# sample\n",
    "test_iter = iter(devset)\n",
    "sample_dir = py.join(output_dir, 'samples_training')\n",
    "py.mkdir(sample_dir)\n",
    "\n",
    "\n",
    "# main loop\n",
    "with train_summary_writer.as_default():\n",
    "    for ep in tqdm.trange(aepochs, desc='Epoch Loop'):\n",
    "        if ep < ep_cnt:\n",
    "            continue\n",
    "\n",
    "        # update epoch counter\n",
    "        ep_cnt.assign_add(1)\n",
    "\n",
    "        # train for an epoch\n",
    "        for A in tqdm.tqdm(A_Set, desc='Inner Epoch Loop', total=len_dataset):\n",
    "            ind = np.random.randint(B_length)\n",
    "            B = B_list[ind]\n",
    "            G_loss_dict, D_loss_dict = train_step(A, B)\n",
    "\n",
    "            # # summary\n",
    "            tl.summary(G_loss_dict, step=G_optimizer.iterations, name='G_losses')\n",
    "            tl.summary(D_loss_dict, step=G_optimizer.iterations, name='D_losses')\n",
    "            tl.summary({'learning rate': G_lr_scheduler.current_learning_rate}, step=G_optimizer.iterations, name='learning rate')\n",
    "\n",
    "            # sample\n",
    "            if G_optimizer.iterations.numpy() % 100 == 0:\n",
    "                A = next(test_iter)\n",
    "                ind = np.random.randint(test_length)\n",
    "                B = test_list[ind]\n",
    "                A,B,A2B, A2B2A = sample(A, B)\n",
    "                img = im.immerge(np.concatenate([A, A2B, A2B2A, B], axis=0), n_rows=2)\n",
    "                im.imwrite(img, py.join(sample_dir, 'iter-%09d.jpg' % G_optimizer.iterations.numpy()))\n",
    "\n",
    "        # save checkpoint\n",
    "        checkpoint.save(ep)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
